def main(argparse_args=None, task=None):
    import os
    from os import path
    import sys

    # sys.path.append('./SLIP')
    sys.path.append('./ResizeRight')
    sys.path.append('./latent-diffusion')

    from secondary_diffusion import SecondaryDiffusionImageNet, SecondaryDiffusionImageNet2 
    from ddim_sampler import DDIMSampler
    from augs import MakeCutouts, MakeCutoutsDango
    from perlin import create_perlin_noise, regen_perlin


    def alpha_sigma_to_t(alpha, sigma):
        return torch.atan2(sigma, alpha) * 2 / math.pi

    root_path = argparse_args.root_path
    initDirPath = f'{root_path}/init_images'
    os.makedirs(initDirPath, exist_ok=1)
    outDirPath = f'{root_path}/images_out'
    os.makedirs(outDirPath, exist_ok=1)
    model_path = f'{root_path}/models'
    os.makedirs(model_path, exist_ok=1)



    #@title ### 2.1 Install and import dependencies
    model_256_downloaded = False
    model_512_downloaded = False
    model_secondary_downloaded = False

    from functools import partial
    import cv2
    import pandas as pd
    import gc
    import io
    import math
    import timm
    from IPython import display
    import lpips
    from PIL import Image, ImageOps
    import requests
    from glob import glob
    import json
    from types import SimpleNamespace
    import torch
    from torch import nn
    from torch.nn import functional as F
    import torchvision.transforms as T
    import torchvision.transforms.functional as TF
    from tqdm.notebook import tqdm
    #sys.path.append('./CLIP')
    sys.path.append('./guided-diffusion')
    import clip
    from resize_right import resize
    # from models import SLIP_VITB16, SLIP, SLIP_VITL16
    from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults
    from datetime import datetime
    import numpy as np
    import matplotlib.pyplot as plt
    import random
    from ipywidgets import Output
    import hashlib


        

    #SuperRes
    import ipywidgets as widgets
    import os
    sys.path.append(".")
    # sys.path.append('./taming-transformers')
    # from taming.models import vqgan # checking correct import from taming
    from torchvision.datasets.utils import download_url

    # sys.path.append("./latent-diffusion")
    # from ldm.util import instantiate_from_config
    # from ldm.models.diffusion.ddim import DDIMSampler
    # from ldm.util import ismap
    #from google.colab import files
    from IPython.display import Image as ipyimg
    from numpy import asarray
    from einops import rearrange, repeat
    import torch, torchvision
    import time
    from omegaconf import OmegaConf
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)


    import torch
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print('Using device:', device)

    if torch.cuda.is_available() and torch.cuda.get_device_capability(device) == (8,0): ## A100 fix thanks to Emad
      print('Disabling CUDNN for A100 gpu', file=sys.stderr)
      torch.backends.cudnn.enabled = False


    #@title 2.2 Define necessary functions

    def fetch(url_or_path):
        if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):
            r = requests.get(url_or_path)
            r.raise_for_status()
            fd = io.BytesIO()
            fd.write(r.content)
            fd.seek(0)
            return fd
        return open(url_or_path, 'rb')


    def parse_prompt(prompt):
        if prompt.startswith('http://') or prompt.startswith('https://'):
            vals = prompt.rsplit(':', 2)
            vals = [vals[0] + ':' + vals[1], *vals[2:]]
        else:
            vals = prompt.rsplit(':', 1)
        vals = vals + ['', '1'][len(vals):]
        return vals[0], float(vals[1])


    def spherical_dist_loss(x, y):
        x = F.normalize(x, dim=-1)
        y = F.normalize(y, dim=-1)
        return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     

    def tv_loss(input):
        """L2 total variation loss, as in Mahendran et al."""
        input = F.pad(input, (0, 1, 0, 1), 'replicate')
        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]
        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]
        return (x_diff**2 + y_diff**2).mean([1, 2, 3])


    def range_loss(input):
        return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])

    stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete

    def do_run():
      seed = args.seed
      print(range(args.start_frame, args.max_frames))
      for frame_num in range(args.start_frame, args.max_frames):
          if stop_on_next_loop:
            break
          
          display.clear_output(wait=True)

          # Print Frame progress if animation mode is on
          if args.animation_mode != "None":
            batchBar = tqdm(range(args.max_frames), desc ="Frames")
            batchBar.n = frame_num
            batchBar.refresh()

          
          # Inits if not video frames
          if args.animation_mode != "Video Input":
            if args.init_image == '':
              init_image = None
            else:
              init_image = args.init_image
            init_scale = args.init_scale
            skip_steps = args.skip_steps

          if args.animation_mode == "2D":
            if args.key_frames:
              angle = args.angle_series[frame_num]
              zoom = args.zoom_series[frame_num]
              translation_x = args.translation_x_series[frame_num]
              translation_y = args.translation_y_series[frame_num]
              print(
                  f'angle: {angle}',
                  f'zoom: {zoom}',
                  f'translation_x: {translation_x}',
                  f'translation_y: {translation_y}',
              )
            
            if frame_num > 0:
              seed = seed + 1          
              if resume_run and frame_num == start_frame:
                img_0 = cv2.imread(batchFolder+f"/{batch_name}({batchNum})_{start_frame-1:04}.png")
              else:
                img_0 = cv2.imread('prevFrame.png')
              center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)
              trans_mat = np.float32(
                  [[1, 0, translation_x],
                  [0, 1, translation_y]]
              )
              rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )
              trans_mat = np.vstack([trans_mat, [0,0,1]])
              rot_mat = np.vstack([rot_mat, [0,0,1]])
              transformation_matrix = np.matmul(rot_mat, trans_mat)
              img_0 = cv2.warpPerspective(
                  img_0,
                  transformation_matrix,
                  (img_0.shape[1], img_0.shape[0]),
                  borderMode=cv2.BORDER_WRAP
              )
              cv2.imwrite('prevFrameScaled.png', img_0)
              init_image = 'prevFrameScaled.png'
              init_scale = args.frames_scale
              skip_steps = args.calc_frames_skip_steps

          if  args.animation_mode == "Video Input":
            seed = seed + 1  
            init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'
            init_scale = args.frames_scale
            skip_steps = args.calc_frames_skip_steps

          loss_values = []
      
          if seed is not None:
              np.random.seed(seed)
              random.seed(seed)
              torch.manual_seed(seed)
              torch.cuda.manual_seed_all(seed)
              torch.backends.cudnn.deterministic = True
      
          target_embeds, weights = [], []
          
          if args.prompts_series is not None and frame_num >= len(args.prompts_series):
            frame_prompt = args.prompts_series[-1]
          elif args.prompts_series is not None:
            frame_prompt = args.prompts_series[frame_num]
          else:
            frame_prompt = []
          
          print(args.image_prompts_series)
          if args.image_prompts_series is not None and frame_num >= len(args.image_prompts_series):
            image_prompt = args.image_prompts_series[-1]
          elif args.image_prompts_series is not None:
            image_prompt = args.image_prompts_series[frame_num]
          else:
            image_prompt = []

          print(f'Frame Prompt: {frame_prompt}')

          model_stats = []
          for clip_model in clip_models:
                cutn = 16
                model_stat = {"clip_model":None,"target_embeds":[],"make_cutouts":None,"weights":[]}
                model_stat["clip_model"] = clip_model
                
                
                for prompt in frame_prompt:
                    txt, weight = parse_prompt(prompt)
                    txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()

                if args.fuzzy_prompt:
                    for i in range(25):
                        model_stat["target_embeds"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0,1))
                        model_stat["weights"].append(weight)
                else:
                    model_stat["target_embeds"].append(txt)
                    model_stat["weights"].append(weight)
            
                if image_prompt:
                  model_stat["make_cutouts"] = MakeCutouts(clip_model.visual.input_resolution, cutn, skip_augs=skip_augs) 
                  for prompt in image_prompt:
                      path, weight = parse_prompt(prompt)
                      img = Image.open(fetch(path)).convert('RGB')
                      img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)
                      batch = model_stat["make_cutouts"](TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))
                      embed = clip_model.encode_image(normalize(batch)).float()
                      if fuzzy_prompt:
                          for i in range(25):
                              model_stat["target_embeds"].append((embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0,1))
                              weights.extend([weight / cutn] * cutn)
                      else:
                          model_stat["target_embeds"].append(embed)
                          model_stat["weights"].extend([weight / cutn] * cutn)
            
                model_stat["target_embeds"] = torch.cat(model_stat["target_embeds"])
                model_stat["weights"] = torch.tensor(model_stat["weights"], device=device)
                if model_stat["weights"].sum().abs() < 1e-3:
                    raise RuntimeError('The weights must not sum to 0.')
                model_stat["weights"] /= model_stat["weights"].sum().abs()
                model_stats.append(model_stat)
      
          init = None
          if init_image is not None:
              init = Image.open(fetch(init_image)).convert('RGB')
              init = init.resize((args.side_x, args.side_y), Image.LANCZOS)
              init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)
          
          if args.perlin_init:
              if args.perlin_mode == 'color':
                  init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)
                  init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)
              elif args.perlin_mode == 'gray':
                init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)
                init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)
              else:
                init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)
                init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)
              # init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)
              init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)
              del init2
      
          cur_t = None
          if init is not None and args.init_scale:
              lpips_model = lpips.LPIPS(net='vgg').to(device)
      
          def cond_fn(x, t, y=None):
              with torch.enable_grad():
                  x_is_NaN = False
                  x = x.detach().requires_grad_()
                  n = x.shape[0]
                  if use_secondary_model is True:
                    alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)
                    sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)
                    cosine_t = alpha_sigma_to_t(alpha, sigma)
                    out = secondary_model(x, cosine_t[None].repeat([n])).pred
                    fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]
                    x_in = out * fac + x * (1 - fac)
                    x_in_grad = torch.zeros_like(x_in)
                  else:
                    my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t
                    out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})
                    fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]
                    x_in = out['pred_xstart'] * fac + x * (1 - fac)
                    x_in_grad = torch.zeros_like(x_in)
                  for model_stat in model_stats:
                    for i in range(args.cutn_batches):
                        t_int = int(t.item())+1 #errors on last step without +1, need to find source
                        #when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'
                        try:
                            input_resolution=model_stat["clip_model"].visual.input_resolution
                        except:
                            input_resolution=224

                        cuts = MakeCutoutsDango(input_resolution,
                                                Overview= args.cut_overview[1000-t_int], 
                                                InnerCrop = args.cut_innercut[1000-t_int], 
                                                IC_Size_Pow=args.cut_ic_pow, 
                                                IC_Grey_P = args.cut_icgray_p[1000-t_int],
                                                animation_mode=args.animation_mode,
                                )
                        clip_in = normalize(cuts(x_in.add(1).div(2)))
                        image_embeds = model_stat["clip_model"].encode_image(clip_in).float()
                        dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat["target_embeds"].unsqueeze(0))
                        dists = dists.view([args.cut_overview[1000-t_int]+args.cut_innercut[1000-t_int], n, -1])
                        losses = dists.mul(model_stat["weights"]).sum(2).mean(0)
                        loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch
                        x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches
                  tv_losses = tv_loss(x_in)
                  if use_secondary_model is True:
                    range_losses = range_loss(out)
                  else:
                    range_losses = range_loss(out['pred_xstart'])
                  sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()
                  loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale

                  if init is not None and args.init_scale:
                      init_losses = lpips_model(x_in, init)
                      loss = loss + init_losses.sum() * args.init_scale
                  x_in_grad += torch.autograd.grad(loss, x_in)[0]
                  if torch.isnan(x_in_grad).any() == False:
                      grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]
                  else:
                    # print("NaN'd")
                    x_is_NaN = True
                    grad = torch.zeros_like(x)
              if args.clamp_grad and x_is_NaN == False:
                  magnitude = grad.square().mean().sqrt()
                  return grad * magnitude.clamp(max=args.clamp_max) / magnitude  #min=-0.02, min=-clamp_max, 
              return grad
      
          if model_config['timestep_respacing'].startswith('ddim'):
              sample_fn = diffusion.ddim_sample_loop_progressive
          else:
              sample_fn = diffusion.p_sample_loop_progressive
        

          image_display = Output()
          for i in range(args.n_batches):
              partial_fname = ""
              if args.animation_mode == 'None':
                display.clear_output(wait=True)
                batchBar = tqdm(range(args.n_batches), desc ="Batches")
                batchBar.n = i
                batchBar.refresh()
              print('')
              display.display(image_display)
              gc.collect()
              torch.cuda.empty_cache()
              cur_t = diffusion.num_timesteps - skip_steps - 1
              total_steps = cur_t

              if perlin_init:
                  init = regen_perlin()

              if model_config['timestep_respacing'].startswith('ddim'):
                  samples = sample_fn(
                      model,
                      (batch_size, 3, args.side_y, args.side_x),
                      clip_denoised=clip_denoised,
                      model_kwargs={},
                      cond_fn=cond_fn,
                      progress=True,
                      skip_timesteps=skip_steps,
                      init_image=init,
                      randomize_class=randomize_class,
                      eta=eta,
                  )
              else:
                  samples = sample_fn(
                      model,
                      (batch_size, 3, args.side_y, args.side_x),
                      clip_denoised=clip_denoised,
                      model_kwargs={},
                      cond_fn=cond_fn,
                      progress=True,
                      skip_timesteps=skip_steps,
                      init_image=init,
                      randomize_class=randomize_class,
                  )
              
              
              # with run_display:
              # display.clear_output(wait=True)
              imgToSharpen = None
              for j, sample in enumerate(samples):    
                cur_t -= 1
                intermediateStep = False
                if task:
                  task.update_state(state='PROGRESS', meta={'current': j, 'total': total_steps, 'partial': partial_fname})
                if args.steps_per_checkpoint is not None:
                    if j % steps_per_checkpoint == 0 and j > 0:
                      intermediateStep = True
                elif j in args.intermediate_saves:
                  intermediateStep = True
                with image_display:
                  if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:
                      for k, image in enumerate(sample['pred_xstart']):
                          # tqdm.write(f'Batch {i}, step {j}, output {k}:')
                          current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')
                          percent = math.ceil(j/total_steps*100)
                          if args.n_batches > 0:
                            #if intermediates are saved to the subfolder, don't append a step or percentage to the name
                            if cur_t == -1 and args.intermediates_in_subfolder is True:
                              save_num = f'{frame_num:04}' if animation_mode != "None" else i
                              filename = f'{args.batch_name}({args.batchNum})_{save_num}.png'
                            else:
                              #If we're working with percentages, append it
                              if args.steps_per_checkpoint is not None:
                                filename = f'{args.batch_name}({args.batchNum})_{i:04}-{percent:02}%.png'
                              # Or else, iIf we're working with specific steps, append those
                              else:
                                filename = f'{args.batch_name}({args.batchNum})_{i:04}-{j:03}.png'
                          image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))
                          if j % args.display_rate == 0 or cur_t == -1:
                            image.save('progress.jpg', subsampling=0, quality=95)
                            #display.clear_output(wait=True)
                            #display.display(display.Image('progress.png'))
                          if args.steps_per_checkpoint is not None:
                            if j % args.steps_per_checkpoint == 0 and j > 0:
                              if args.intermediates_in_subfolder is True:
                                partial_fname = f'{partialFolder}/{filename}'
                                image.save(partial_fname)
                              else:
                                image.save(f'{batchFolder}/{filename}')
                          else:
                            if j in args.intermediate_saves:
                              if args.intermediates_in_subfolder is True:
                                partial_fname = f'{partialFolder}/{filename}'
                                image.save(partial_fname)
                              else:
                                image.save(f'{batchFolder}/{filename}')
                          if cur_t == -1:
                            if frame_num == 0:
                              save_settings()
                            if args.animation_mode != "None":
                              image.save('prevFrame.jpg', subsampling=0, quality=95)
                            if args.sharpen_preset != "Off" and animation_mode == "None":
                              imgToSharpen = image
                              if args.keep_unsharp is True:
                                image.save(f'{unsharpenFolder}/{filename}')
                            else:
                              image.save(f'{batchFolder}/{filename}')
                            # if frame_num != args.max_frames-1:
                            #   display.clear_output()

              with image_display:   
                if args.sharpen_preset != "Off" and animation_mode == "None":
                  print('Starting Diffusion Sharpening...')
                  do_superres(imgToSharpen, f'{batchFolder}/{filename}')
                  display.clear_output()
              
              plt.plot(np.array(loss_values), 'r')

    def save_settings():
      setting_list = {
        'text_prompts': text_prompts,
        'image_prompts': image_prompts,
        'clip_guidance_scale': clip_guidance_scale,
        'tv_scale': tv_scale,
        'range_scale': range_scale,
        'sat_scale': sat_scale,
        # 'cutn': cutn,
        'cutn_batches': cutn_batches,
        'max_frames': max_frames,
        'interp_spline': interp_spline,
        # 'rotation_per_frame': rotation_per_frame,
        'init_image': init_image,
        'init_scale': init_scale,
        'skip_steps': skip_steps,
        # 'zoom_per_frame': zoom_per_frame,
        'frames_scale': frames_scale,
        'frames_skip_steps': frames_skip_steps,
        'perlin_init': perlin_init,
        'perlin_mode': perlin_mode,
        'skip_augs': skip_augs,
        'randomize_class': randomize_class,
        'clip_denoised': clip_denoised,
        'clamp_grad': clamp_grad,
        'clamp_max': clamp_max,
        'seed': seed,
        'fuzzy_prompt': fuzzy_prompt,
        'rand_mag': rand_mag,
        'eta': eta,
        'width': width_height[0],
        'height': width_height[1],
        'diffusion_model': diffusion_model,
        'use_secondary_model': use_secondary_model,
        'steps': steps,
        'diffusion_steps': diffusion_steps,
        'ViTB32': ViTB32,
        'ViTB16': ViTB16,
        'RN101': RN101,
        'RN50': RN50,
        'RN50x4': RN50x4,
        'RN50x16': RN50x16,
        'cut_overview': str(cut_overview),
        'cut_innercut': str(cut_innercut),
        'cut_ic_pow': cut_ic_pow,
        'cut_icgray_p': str(cut_icgray_p),
        'key_frames': key_frames,
        'max_frames': max_frames,
        'angle': angle,
        'zoom': zoom,
        'translation_x': translation_x,
        'translation_y': translation_y,
        'video_init_path':video_init_path,
        'extract_nth_frame':extract_nth_frame,
      }
      # print('Settings:', setting_list)
      with open(f"{batchFolder}/{batch_name}({batchNum})_settings.txt", "w+") as f:   #save settings
        json.dump(setting_list, f, ensure_ascii=False, indent=4)

    #@title 2.4 SuperRes Define

    def download_models():
        # this is the small bsr light model
        url_conf = 'https://heibox.uni-heidelberg.de/f/31a76b13ea27482981b4/?dl=1'
        url_ckpt = 'https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1'

        path_conf = f'{model_path}/superres/'
        path_ckpt = f'{model_path}/superres/'

        download_url(url_conf, path_conf, 'project.yaml')
        download_url(url_ckpt, path_ckpt, 'last.ckpt')

        path_conf = path_conf + 'project.yaml' # fix it
        path_ckpt = path_ckpt + 'last.ckpt' # fix it
        return path_conf, path_ckpt
      

    def load_model_from_config(config, ckpt):
        print(f"Loading model from {ckpt}")
        pl_sd = torch.load(ckpt, map_location="cpu")
        global_step = pl_sd["global_step"]
        sd = pl_sd["state_dict"]
        model = instantiate_from_config(config.model)
        m, u = model.load_state_dict(sd, strict=False)
        model.cuda()
        model.eval()
        return {"model": model}, global_step


    def get_model(mode):
        path_conf, path_ckpt = download_models()
        config = OmegaConf.load(path_conf)
        model, step = load_model_from_config(config, path_ckpt)
        return model


    def get_custom_cond(mode):
        dest = "data/example_conditioning"

        if mode == "superresolution":
            uploaded_img = files.upload()
            filename = next(iter(uploaded_img))
            name, filetype = filename.split(".") # todo assumes just one dot in name !
            os.rename(f"{filename}", f"{dest}/{mode}/custom_{name}.{filetype}")

        elif mode == "text_conditional":
            w = widgets.Text(value='A cake with cream!', disabled=True)
            display.display(w)

            with open(f"{dest}/{mode}/custom_{w.value[:20]}.txt", 'w') as f:
                f.write(w.value)

        elif mode == "class_conditional":
            w = widgets.IntSlider(min=0, max=1000)
            display.display(w)
            with open(f"{dest}/{mode}/custom.txt", 'w') as f:
                f.write(w.value)

        else:
            raise NotImplementedError(f"cond not implemented for mode{mode}")


    def get_cond_options(mode):
        path = "data/example_conditioning"
        path = os.path.join(path, mode)
        onlyfiles = [f for f in sorted(os.listdir(path))]
        return path, onlyfiles


    def select_cond_path(mode):
        path = "data/example_conditioning"  # todo
        path = os.path.join(path, mode)
        onlyfiles = [f for f in sorted(os.listdir(path))]

        selected = widgets.RadioButtons(
            options=onlyfiles,
            description='Select conditioning:',
            disabled=False
        )
        display.display(selected)
        selected_path = os.path.join(path, selected.value)
        return selected_path


    def get_cond(mode, img):
        example = dict()
        if mode == "superresolution":
            up_f = 4
            # visualize_cond_img(selected_path)

            c = img
            c = torch.unsqueeze(torchvision.transforms.ToTensor()(c), 0)
            c_up = torchvision.transforms.functional.resize(c, size=[up_f * c.shape[2], up_f * c.shape[3]], antialias=True)
            c_up = rearrange(c_up, '1 c h w -> 1 h w c')
            c = rearrange(c, '1 c h w -> 1 h w c')
            c = 2. * c - 1.

            c = c.to(torch.device("cuda"))
            example["LR_image"] = c
            example["image"] = c_up

        return example


    def visualize_cond_img(path):
        display.display(ipyimg(filename=path))


    def sr_run(model, img, task, custom_steps, eta, resize_enabled=False, classifier_ckpt=None, global_step=None):
        # global stride

        example = get_cond(task, img)

        save_intermediate_vid = False
        n_runs = 1
        masked = False
        guider = None
        ckwargs = None
        mode = 'ddim'
        ddim_use_x0_pred = False
        temperature = 1.
        eta = eta
        make_progrow = True
        custom_shape = None

        height, width = example["image"].shape[1:3]
        split_input = height >= 128 and width >= 128

        if split_input:
            ks = 128
            stride = 64
            vqf = 4  #
            model.split_input_params = {"ks": (ks, ks), "stride": (stride, stride),
                                        "vqf": vqf,
                                        "patch_distributed_vq": True,
                                        "tie_braker": False,
                                        "clip_max_weight": 0.5,
                                        "clip_min_weight": 0.01,
                                        "clip_max_tie_weight": 0.5,
                                        "clip_min_tie_weight": 0.01}
        else:
            if hasattr(model, "split_input_params"):
                delattr(model, "split_input_params")

        invert_mask = False

        x_T = None
        for n in range(n_runs):
            if custom_shape is not None:
                x_T = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)
                x_T = repeat(x_T, '1 c h w -> b c h w', b=custom_shape[0])

            logs = make_convolutional_sample(example, model,
                                            mode=mode, custom_steps=custom_steps,
                                            eta=eta, swap_mode=False , masked=masked,
                                            invert_mask=invert_mask, quantize_x0=False,
                                            custom_schedule=None, decode_interval=10,
                                            resize_enabled=resize_enabled, custom_shape=custom_shape,
                                            temperature=temperature, noise_dropout=0.,
                                            corrector=guider, corrector_kwargs=ckwargs, x_T=x_T, save_intermediate_vid=save_intermediate_vid,
                                            make_progrow=make_progrow,ddim_use_x0_pred=ddim_use_x0_pred
                                            )
        return logs


    @torch.no_grad()
    def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,
                        mask=None, x0=None, quantize_x0=False, img_callback=None,
                        temperature=1., noise_dropout=0., score_corrector=None,
                        corrector_kwargs=None, x_T=None, log_every_t=None
                        ):

        ddim = DDIMSampler(model)
        bs = shape[0]  # dont know where this comes from but wayne
        shape = shape[1:]  # cut batch dim
        # print(f"Sampling with eta = {eta}; steps: {steps}")
        samples, intermediates = ddim.sample(steps, batch_size=bs, shape=shape, conditioning=cond, callback=callback,
                                            normals_sequence=normals_sequence, quantize_x0=quantize_x0, eta=eta,
                                            mask=mask, x0=x0, temperature=temperature, verbose=False,
                                            score_corrector=score_corrector,
                                            corrector_kwargs=corrector_kwargs, x_T=x_T)

        return samples, intermediates


    @torch.no_grad()
    def make_convolutional_sample(batch, model, mode="vanilla", custom_steps=None, eta=1.0, swap_mode=False, masked=False,
                                  invert_mask=True, quantize_x0=False, custom_schedule=None, decode_interval=1000,
                                  resize_enabled=False, custom_shape=None, temperature=1., noise_dropout=0., corrector=None,
                                  corrector_kwargs=None, x_T=None, save_intermediate_vid=False, make_progrow=True,ddim_use_x0_pred=False):
        log = dict()

        z, c, x, xrec, xc = model.get_input(batch, model.first_stage_key,
                                            return_first_stage_outputs=True,
                                            force_c_encode=not (hasattr(model, 'split_input_params')
                                                                and model.cond_stage_key == 'coordinates_bbox'),
                                            return_original_cond=True)

        log_every_t = 1 if save_intermediate_vid else None

        if custom_shape is not None:
            z = torch.randn(custom_shape)
            # print(f"Generating {custom_shape[0]} samples of shape {custom_shape[1:]}")

        z0 = None

        log["input"] = x
        log["reconstruction"] = xrec

        if ismap(xc):
            log["original_conditioning"] = model.to_rgb(xc)
            if hasattr(model, 'cond_stage_key'):
                log[model.cond_stage_key] = model.to_rgb(xc)

        else:
            log["original_conditioning"] = xc if xc is not None else torch.zeros_like(x)
            if model.cond_stage_model:
                log[model.cond_stage_key] = xc if xc is not None else torch.zeros_like(x)
                if model.cond_stage_key =='class_label':
                    log[model.cond_stage_key] = xc[model.cond_stage_key]

        with model.ema_scope("Plotting"):
            t0 = time.time()
            img_cb = None

            sample, intermediates = convsample_ddim(model, c, steps=custom_steps, shape=z.shape,
                                                    eta=eta,
                                                    quantize_x0=quantize_x0, img_callback=img_cb, mask=None, x0=z0,
                                                    temperature=temperature, noise_dropout=noise_dropout,
                                                    score_corrector=corrector, corrector_kwargs=corrector_kwargs,
                                                    x_T=x_T, log_every_t=log_every_t)
            t1 = time.time()

            if ddim_use_x0_pred:
                sample = intermediates['pred_x0'][-1]

        x_sample = model.decode_first_stage(sample)

        try:
            x_sample_noquant = model.decode_first_stage(sample, force_not_quantize=True)
            log["sample_noquant"] = x_sample_noquant
            log["sample_diff"] = torch.abs(x_sample_noquant - x_sample)
        except:
            pass

        log["sample"] = x_sample
        log["time"] = t1 - t0

        return log

    sr_diffMode = 'superresolution'
    sr_model = get_model('superresolution') if argparse_args.sharpen_preset != "Off" else None


    def do_superres(img, filepath):

      if args.sharpen_preset == 'Faster':
          sr_diffusion_steps = "25" 
          sr_pre_downsample = '1/2' 
      if args.sharpen_preset == 'Fast':
          sr_diffusion_steps = "100" 
          sr_pre_downsample = '1/2' 
      if args.sharpen_preset == 'Slow':
          sr_diffusion_steps = "25" 
          sr_pre_downsample = 'None' 
      if args.sharpen_preset == 'Very Slow':
          sr_diffusion_steps = "100" 
          sr_pre_downsample = 'None' 


      sr_post_downsample = 'Original Size'
      sr_diffusion_steps = int(sr_diffusion_steps)
      sr_eta = 1.0 
      sr_downsample_method = 'Lanczos' 

      gc.collect()
      torch.cuda.empty_cache()

      im_og = img
      width_og, height_og = im_og.size

      #Downsample Pre
      if sr_pre_downsample == '1/2':
        downsample_rate = 2
      elif sr_pre_downsample == '1/4':
        downsample_rate = 4
      else:
        downsample_rate = 1

      width_downsampled_pre = width_og//downsample_rate
      height_downsampled_pre = height_og//downsample_rate

      if downsample_rate != 1:
        # print(f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')
        im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)
        # im_og.save('/content/temp.png')
        # filepath = '/content/temp.png'

      logs = sr_run(sr_model["model"], im_og, sr_diffMode, sr_diffusion_steps, sr_eta)

      sample = logs["sample"]
      sample = sample.detach().cpu()
      sample = torch.clamp(sample, -1., 1.)
      sample = (sample + 1.) / 2. * 255
      sample = sample.numpy().astype(np.uint8)
      sample = np.transpose(sample, (0, 2, 3, 1))
      a = Image.fromarray(sample[0])

      #Downsample Post
      if sr_post_downsample == '1/2':
        downsample_rate = 2
      elif sr_post_downsample == '1/4':
        downsample_rate = 4
      else:
        downsample_rate = 1

      width, height = a.size
      width_downsampled_post = width//downsample_rate
      height_downsampled_post = height//downsample_rate

      if sr_downsample_method == 'Lanczos':
        aliasing = Image.LANCZOS
      else:
        aliasing = Image.NEAREST

      if downsample_rate != 1:
        # print(f'Downsampling from [{width}, {height}] to [{width_downsampled_post}, {height_downsampled_post}]')
        a = a.resize((width_downsampled_post, height_downsampled_post), aliasing)
      elif sr_post_downsample == 'Original Size':
        # print(f'Downsampling from [{width}, {height}] to Original Size [{width_og}, {height_og}]')
        a = a.resize((width_og, height_og), aliasing)

      display.display(a)
      a.save(filepath)
      return
      print(f'Processing finished!')


    # # 3. Diffusion and CLIP model settings

    #@markdown ####**Models Settings:**
    diffusion_model = "512x512_diffusion_uncond_finetune_008100" #@param ["256x256_diffusion_uncond", "512x512_diffusion_uncond_finetune_008100"]
    use_secondary_model = True #@param {type: 'boolean'}

    timestep_respacing = '50' # param ['25','50','100','150','250','500','1000','ddim25','ddim50', 'ddim75', 'ddim100','ddim150','ddim250','ddim500','ddim1000']  
    diffusion_steps = 1000 # param {type: 'number'}
    use_checkpoint = True #@param {type: 'boolean'}
    ViTB32 = True #@param{type:"boolean"}
    ViTB16 = True #@param{type:"boolean"}
    RN101 = False #@param{type:"boolean"}
    RN50 = True #@param{type:"boolean"}
    RN50x4 = False #@param{type:"boolean"}
    RN50x16 = False #@param{type:"boolean"}
    SLIPB16 = False # param{type:"boolean"}
    SLIPL16 = False # param{type:"boolean"}

    #@markdown If you're having issues with model downloads, check this to compare SHA's:
    check_model_SHA = False #@param{type:"boolean"}

    model_256_SHA = '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a'
    model_512_SHA = '9c111ab89e214862b76e1fa6a1b3f1d329b1a88281885943d2cdbe357ad57648'
    model_secondary_SHA = '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a'

    model_256_link = 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt'
    model_512_link = 'http://batbot.tv/ai/models/guided-diffusion/512x512_diffusion_uncond_finetune_008100.pt'
    model_secondary_link = 'https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet_2.pth'

    model_256_path = f'{model_path}/256x256_diffusion_uncond.pt'
    model_512_path = f'{model_path}/512x512_diffusion_uncond_finetune_008100.pt'
    model_secondary_path = f'{model_path}/secondary_model_imagenet_2.pth'

    # Download the diffusion model
    if diffusion_model == '256x256_diffusion_uncond':
      if os.path.exists(model_256_path) and check_model_SHA:
        print('Checking 256 Diffusion File')
        with open(model_256_path,"rb") as f:
            bytes = f.read() 
            hash = hashlib.sha256(bytes).hexdigest()
        if hash == model_256_SHA:
          print('256 Model SHA matches')
          model_256_downloaded = True
        else: 
          print("256 Model SHA doesn't match, redownloading...")
          if argparse_args.setup:
              download_url(model_256_link, model_path, '256x256_diffusion_uncond.pt')
          model_256_downloaded = True
      elif os.path.exists(model_256_path) and not check_model_SHA or model_256_downloaded == True:
        print('256 Model already downloaded, check check_model_SHA if the file is corrupt')
      else:  
        if argparse_args.setup:
            download_url(model_256_link, model_path, '256x256_diffusion_uncond.pt')
        model_256_downloaded = True
    elif diffusion_model == '512x512_diffusion_uncond_finetune_008100':
      if os.path.exists(model_512_path) and check_model_SHA:
        print('Checking 512 Diffusion File')
        with open(model_512_path,"rb") as f:
            bytes = f.read() 
            hash = hashlib.sha256(bytes).hexdigest()
        if hash == model_512_SHA:
          print('512 Model SHA matches')
          model_512_downloaded = True
        else:  
          print("512 Model SHA doesn't match, redownloading...")
          if argparse_args.setup:
              download_url(model_512_link, model_path, '512x512_diffusion_uncond_finetune_008100.pt')
          model_512_downloaded = True
      elif os.path.exists(model_512_path) and not check_model_SHA or model_512_downloaded == True:
        print('512 Model already downloaded, check check_model_SHA if the file is corrupt')
      else:  
        if argparse_args.setup:
            download_url(model_512_link, model_path, '512x512_diffusion_uncond_finetune_008100.pt')
        model_512_downloaded = True


    # Download the secondary diffusion model v2
    if use_secondary_model == True:
      if os.path.exists(model_secondary_path) and check_model_SHA:
        print('Checking Secondary Diffusion File')
        with open(model_secondary_path,"rb") as f:
            bytes = f.read() 
            hash = hashlib.sha256(bytes).hexdigest()
        if hash == model_secondary_SHA:
          print('Secondary Model SHA matches')
          model_secondary_downloaded = True
        else:  
          print("Secondary Model SHA doesn't match, redownloading...")
          if argparse_args.setup:
              download_url(model_secondary_link, model_path, 'secondary_model_imagenet_2.pth')
          model_secondary_downloaded = True
      elif os.path.exists(model_secondary_path) and not check_model_SHA or model_secondary_downloaded == True:
        print('Secondary Model already downloaded, check check_model_SHA if the file is corrupt')
      else:  
        if argparse_args.setup:
            download_url(model_secondary_link, model_path, 'secondary_model_imagenet_2.pth')
        model_secondary_downloaded = True

    model_config = model_and_diffusion_defaults()
    if diffusion_model == '512x512_diffusion_uncond_finetune_008100':
        model_config.update({
            'attention_resolutions': '32, 16, 8',
            'class_cond': False,
            'diffusion_steps': diffusion_steps,
            'rescale_timesteps': True,
            'timestep_respacing': timestep_respacing,
            'image_size': 512,
            'learn_sigma': True,
            'noise_schedule': 'linear',
            'num_channels': 256,
            'num_head_channels': 64,
            'num_res_blocks': 2,
            'resblock_updown': True,
            'use_checkpoint': use_checkpoint,
            'use_fp16': True,
            'use_scale_shift_norm': True,
        })
    elif diffusion_model == '256x256_diffusion_uncond':
        model_config.update({
            'attention_resolutions': '32, 16, 8',
            'class_cond': False,
            'diffusion_steps': diffusion_steps,
            'rescale_timesteps': True,
            'timestep_respacing': timestep_respacing,
            'image_size': 256,
            'learn_sigma': True,
            'noise_schedule': 'linear',
            'num_channels': 256,
            'num_head_channels': 64,
            'num_res_blocks': 2,
            'resblock_updown': True,
            'use_checkpoint': use_checkpoint,
            'use_fp16': True,
            'use_scale_shift_norm': True,
        })

    model_default = model_config['image_size']

    secondary_model = SecondaryDiffusionImageNet2()
    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))
    secondary_model.eval().requires_grad_(False).to(device)


    clip_dict = {'ViT-B/32': ViTB32, 'ViT-B/16': ViTB16, 'RN50': RN50, 'RN50x4': RN50x4, 'RN50x16': RN50x16, 'RN101': RN101}
    clip_models = [clip.load(clip_name, jit=False)[0].eval().requires_grad_(False).to(device) for clip_name in clip_dict if clip_dict[clip_name]]

    if SLIPB16:
      SLIPB16model = SLIP_VITB16(ssl_mlp_dim=4096, ssl_emb_dim=256)
      if argparse_args.setup:
          if not os.path.exists(f'{model_path}/slip_base_100ep.pt'):
            get_ipython().system('wget https://dl.fbaipublicfiles.com/slip/slip_base_100ep.pt -P {model_path}')
      sd = torch.load(f'{model_path}/slip_base_100ep.pt')
      real_sd = {}
      for k, v in sd['state_dict'].items():
        real_sd['.'.join(k.split('.')[1:])] = v
      del sd
      SLIPB16model.load_state_dict(real_sd)
      SLIPB16model.requires_grad_(False).eval().to(device)

      clip_models.append(SLIPB16model)

    if SLIPL16:
      SLIPL16model = SLIP_VITL16(ssl_mlp_dim=4096, ssl_emb_dim=256)
      if argparse_args.setup:
          if not os.path.exists(f'{model_path}/slip_large_100ep.pt'):
            get_ipython().system('wget https://dl.fbaipublicfiles.com/slip/slip_large_100ep.pt -P {model_path}')
      sd = torch.load(f'{model_path}/slip_large_100ep.pt')
      real_sd = {}
      for k, v in sd['state_dict'].items():
        real_sd['.'.join(k.split('.')[1:])] = v
      del sd
      SLIPL16model.load_state_dict(real_sd)
      SLIPL16model.requires_grad_(False).eval().to(device)

      clip_models.append(SLIPL16model)

    normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])

    # # 4. Settings


    #@markdown ####**Basic Settings:**
    batch_name = argparse_args.out_name #@param{type: 'string'}
    steps = argparse_args.steps #@param [25,50,100,150,250,500,1000]{type: 'raw', allow-input: true}
    width_height = [argparse_args.width, argparse_args.height]#@param{type: 'raw'}
    clip_guidance_scale = 5000 #@param{type: 'number'}
    tv_scale =  0#@param{type: 'number'}
    range_scale =   150#@param{type: 'number'}
    sat_scale =   0#@param{type: 'number'}
    cutn_batches = 4  #@param{type: 'number'}
    skip_augs = False#@param{type: 'boolean'}

    #@markdown ---

    #@markdown ####**Init Settings:**
    init_image = argparse_args.init_image #@param{type: 'string'}
    init_scale = 1000 #@param{type: 'integer'}
    skip_steps = argparse_args.skip_steps if argparse_args.skip_steps is not None else (steps // 2 if init_image is not None else 0)

    #Get corrected sizes
    side_x = (width_height[0]//64)*64;
    side_y = (width_height[1]//64)*64;
    if side_x != width_height[0] or side_y != width_height[1]:
      print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')

    #Update Model Settings
    timestep_respacing = f'ddim{steps}'
    diffusion_steps = (1000//steps)*steps if steps < 1000 else steps
    model_config.update({
        'timestep_respacing': timestep_respacing,
        'diffusion_steps': diffusion_steps,
    })

    #Make folder for batch
    batchFolder = f'{outDirPath}/{batch_name}'
    os.makedirs(batchFolder, exist_ok=1)


    # ###Animation Settings

    # In[13]:


    #@markdown ####**Animation Mode:**
    animation_mode = "None" #@param['None', '2D', 'Video Input']
    #@markdown *For animation, you probably want to turn `cutn_batches` to 1 to make it quicker.*


    #@markdown ---

    #@markdown ####**Video Input Settings:**
    video_init_path = "/content/training.mp4" #@param {type: 'string'}
    extract_nth_frame = 2 #@param {type:"number"} 

    if animation_mode == "Video Input":
        videoFramesFolder = f'/content/videoFrames'
        os.makedirs(videoFramesFolder, exist_ok=True)
        print(f"Exporting Video Frames (1 every {extract_nth_frame})...")
        if argparse_args.setup:
            try:
                get_ipython().system('rm {videoFramesFolder}/*.jpg')
            except:
                print('')
        vf = f'"select=not(mod(n\,{extract_nth_frame}))"'
        if argparse_args.setup:
            get_ipython().system('ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg')


    #@markdown ---

    #@markdown ####**2D Animation Settings:**
    #@markdown `zoom` is a multiplier of dimensions, 1 is no zoom.

    key_frames = True #@param {type:"boolean"}
    max_frames = 10000#@param {type:"number"}

    if animation_mode == "Video Input":
      max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))

    interp_spline = 'Linear' #Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:"string"}
    angle = "0:(0)"#@param {type:"string"}
    zoom = "0: (1), 10: (1.05)"#@param {type:"string"}
    translation_x = "0: (0)"#@param {type:"string"}
    translation_y = "0: (0)"#@param {type:"string"}

    #@markdown ---

    #@markdown ####**Coherency Settings:**
    #@markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.
    frames_scale = 1500 #@param{type: 'integer'}
    #@markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.
    frames_skip_steps = '60%' #@param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}


    def parse_key_frames(string, prompt_parser=None):
        """Given a string representing frame numbers paired with parameter values at that frame,
        return a dictionary with the frame numbers as keys and the parameter values as the values.

        Parameters
        ----------
        string: string
            Frame numbers paired with parameter values at that frame number, in the format
            'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'
        prompt_parser: function or None, optional
            If provided, prompt_parser will be applied to each string of parameter values.
        
        Returns
        -------
        dict
            Frame numbers as keys, parameter values at that frame number as values

        Raises
        ------
        RuntimeError
            If the input string does not match the expected format.
        
        Examples
        --------
        >>> parse_key_frames("10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)")
        {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}

        >>> parse_key_frames("10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)", prompt_parser=lambda x: x.lower()))
        {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}
        """
        import re
        pattern = r'((?P<frame>[0-9]+):[\s]*[\(](?P<param>[\S\s]*?)[\)])'
        frames = dict()
        for match_object in re.finditer(pattern, string):
            frame = int(match_object.groupdict()['frame'])
            param = match_object.groupdict()['param']
            if prompt_parser:
                frames[frame] = prompt_parser(param)
            else:
                frames[frame] = param

        if frames == {} and len(string) != 0:
            raise RuntimeError('Key Frame string not correctly formatted')
        return frames

    def get_inbetweens(key_frames, integer=False):
        """Given a dict with frame numbers as keys and a parameter value as values,
        return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.
        Any values not provided in the input dict are calculated by linear interpolation between
        the values of the previous and next provided frames. If there is no previous provided frame, then
        the value is equal to the value of the next provided frame, or if there is no next provided frame,
        then the value is equal to the value of the previous provided frame. If no frames are provided,
        all frame values are NaN.

        Parameters
        ----------
        key_frames: dict
            A dict with integer frame numbers as keys and numerical values of a particular parameter as values.
        integer: Bool, optional
            If True, the values of the output series are converted to integers.
            Otherwise, the values are floats.
        
        Returns
        -------
        pd.Series
            A Series with length max_frames representing the parameter values for each frame.
        
        Examples
        --------
        >>> max_frames = 5
        >>> get_inbetweens({1: 5, 3: 6})
        0    5.0
        1    5.0
        2    5.5
        3    6.0
        4    6.0
        dtype: float64

        >>> get_inbetweens({1: 5, 3: 6}, integer=True)
        0    5
        1    5
        2    5
        3    6
        4    6
        dtype: int64
        """
        key_frame_series = pd.Series([np.nan for a in range(max_frames)])

        for i, value in key_frames.items():
            key_frame_series[i] = value
        key_frame_series = key_frame_series.astype(float)
        
        interp_method = interp_spline

        if interp_method == 'Cubic' and len(key_frames.items()) <=3:
          interp_method = 'Quadratic'
        
        if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:
          interp_method = 'Linear'
          
        
        key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]
        key_frame_series[max_frames-1] = key_frame_series[key_frame_series.last_valid_index()]
        # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')
        key_frame_series = key_frame_series.interpolate(method=interp_method.lower(),limit_direction='both')
        if integer:
            return key_frame_series.astype(int)
        return key_frame_series


    def split_prompts(prompts):
      prompt_series = pd.Series([np.nan for a in range(max_frames)])
      for i, prompt in prompts.items():
        prompt_series[i] = prompt
      # prompt_series = prompt_series.astype(str)
      prompt_series = prompt_series.ffill().bfill()
      return prompt_series


    if key_frames:
        try:
            angle_series = get_inbetweens(parse_key_frames(angle))
        except RuntimeError as e:
            print(
                "WARNING: You have selected to use key frames, but you have not "
                "formatted `angle` correctly for key frames.\n"
                "Attempting to interpret `angle` as "
                f'"0: ({angle})"\n'
                "Please read the instructions to find out how to use key frames "
                "correctly.\n"
            )
            angle = f"0: ({angle})"
            angle_series = get_inbetweens(parse_key_frames(angle))

        try:
            zoom_series = get_inbetweens(parse_key_frames(zoom))
        except RuntimeError as e:
            print(
                "WARNING: You have selected to use key frames, but you have not "
                "formatted `zoom` correctly for key frames.\n"
                "Attempting to interpret `zoom` as "
                f'"0: ({zoom})"\n'
                "Please read the instructions to find out how to use key frames "
                "correctly.\n"
            )
            zoom = f"0: ({zoom})"
            zoom_series = get_inbetweens(parse_key_frames(zoom))

        try:
            translation_x_series = get_inbetweens(parse_key_frames(translation_x))
        except RuntimeError as e:
            print(
                "WARNING: You have selected to use key frames, but you have not "
                "formatted `translation_x` correctly for key frames.\n"
                "Attempting to interpret `translation_x` as "
                f'"0: ({translation_x})"\n'
                "Please read the instructions to find out how to use key frames "
                "correctly.\n"
            )
            translation_x = f"0: ({translation_x})"
            translation_x_series = get_inbetweens(parse_key_frames(translation_x))

        try:
            translation_y_series = get_inbetweens(parse_key_frames(translation_y))
        except RuntimeError as e:
            print(
                "WARNING: You have selected to use key frames, but you have not "
                "formatted `translation_y` correctly for key frames.\n"
                "Attempting to interpret `translation_y` as "
                f'"0: ({translation_y})"\n'
                "Please read the instructions to find out how to use key frames "
                "correctly.\n"
            )
            translation_y = f"0: ({translation_y})"
            translation_y_series = get_inbetweens(parse_key_frames(translation_y))

    else:
        angle = float(angle)
        zoom = float(zoom)
        translation_x = float(translation_x)
        translation_y = float(translation_y)


    # ### Extra Settings
    #  Partial Saves, Diffusion Sharpening, Advanced Settings, Cutn Scheduling

    # In[14]:


    #@markdown ####**Saving:**

    intermediate_saves = argparse_args.inter_saves  #@param{type: 'raw'}
    intermediates_in_subfolder = True #@param{type: 'boolean'}
    #@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps 

    #@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.

    #@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)


    if type(intermediate_saves) is not list:
      if intermediate_saves:
        steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))
        steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1
        print(f'Will save every {steps_per_checkpoint} steps')
      else:
        steps_per_checkpoint = steps+10
    else:
      steps_per_checkpoint = None

    if intermediate_saves and intermediates_in_subfolder is True:
      partialFolder = f'{batchFolder}/partials'
      os.makedirs(partialFolder, exist_ok=True)

      #@markdown ---

    #@markdown ####**SuperRes Sharpening:**
    #@markdown *Sharpen each image using latent-diffusion. Does not run in animation mode. `keep_unsharp` will save both versions.*
    sharpen_preset = argparse_args.sharpen_preset #@param ['Off', 'Faster', 'Fast', 'Slow', 'Very Slow']
    keep_unsharp = True #@param{type: 'boolean'}

    if sharpen_preset != 'Off' and keep_unsharp is True:
      unsharpenFolder = f'{batchFolder}/unsharpened'
      os.makedirs(unsharpenFolder, exist_ok=True)


      #@markdown ---

    #@markdown ####**Advanced Settings:**
    #@markdown *There are a few extra advanced settings available if you double click this cell.*

    #@markdown *Perlin init will replace your init, so uncheck if using one.*

    perlin_init = False  #@param{type: 'boolean'}
    perlin_mode = 'mixed' #@param ['mixed', 'color', 'gray']
    set_seed = 'random_seed' #@param{type: 'string'}
    eta = 0.8#@param{type: 'number'}
    clamp_grad = True #@param{type: 'boolean'}
    clamp_max = 0.05 #@param{type: 'number'}


    ### EXTRA ADVANCED SETTINGS:
    randomize_class = True
    clip_denoised = False
    fuzzy_prompt = False
    rand_mag = 0.05


    #@markdown ---

    #@markdown ####**Cutn Scheduling:**
    #@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000

    #@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.

    cut_overview = "[12]*400+[4]*600" #@param {type: 'string'}       
    cut_innercut ="[4]*400+[12]*600"#@param {type: 'string'}  
    cut_ic_pow = 1#@param {type: 'number'}  
    cut_icgray_p = "[0.2]*400+[0]*600"#@param {type: 'string'}  


    # ###Prompts
    # `animation_mode: None` will only use the first set. `animation_mode: 2D / Video` will run through them per the set frames and hold on the last one.

    # In[15]:


    text_prompts = {
        0: argparse_args.text.split("|"),
    #"A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood by greg rutkowski and thomas kinkade, Trending on artstation."],
        # 100: ["This set of prompts start at frame 100", "This prompt has weight five:5"],
    }

    image_prompts = {
        
        #0:['base_images/tree_of_life/tree_new_1.jpeg',],
    }
    if argparse_args.setup:
      sys.exit()


    # # 5. Diffuse!
    #@title Do the Run!
    #@markdown `n_batches` ignored with animation modes.
    display_rate =  10 #@param{type: 'number'}
    n_batches =  1 #@param{type: 'number'}
    batch_size = 1 


    resume_run = False #@param{type: 'boolean'}
    run_to_resume = 'latest' #@param{type: 'string'}
    resume_from_frame = 'latest' #@param{type: 'string'}
    retain_overwritten_frames = False #@param{type: 'boolean'}
    if retain_overwritten_frames is True:
      retainFolder = f'{batchFolder}/retained'
      os.makedirs(retainFolder, exist_ok=True)


    skip_step_ratio = int(frames_skip_steps.rstrip("%")) / 100
    calc_frames_skip_steps = math.floor(steps * skip_step_ratio)

    if steps <= calc_frames_skip_steps:
      sys.exit("ERROR: You can't skip more steps than your total steps")

    if resume_run:
      if run_to_resume == 'latest':
        try:
          batchNum
        except:
          batchNum = len(glob(f"{batchFolder}/{batch_name}(*)_settings.txt"))-1
      else:
        batchNum = int(run_to_resume)
      if resume_from_frame == 'latest':
        start_frame = len(glob(batchFolder+f"/{batch_name}({batchNum})_*.png"))
      else:
        start_frame = int(resume_from_frame)+1
        if retain_overwritten_frames is True:
          existing_frames = len(glob(batchFolder+f"/{batch_name}({batchNum})_*.png"))
          frames_to_save = existing_frames - start_frame
          print(f'Moving {frames_to_save} frames to the Retained folder')
          move_files(start_frame, existing_frames, batchFolder, retainFolder)
    else:
      start_frame = 0
      batchNum = len(glob(batchFolder+"/*.txt"))
      while path.isfile(f"{batchFolder}/{batch_name}({batchNum})_settings.txt") is True or path.isfile(f"{batchFolder}/{batch_name}-{batchNum}_settings.txt") is True:
        batchNum += 1

    print(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')

    if set_seed == 'random_seed':
        random.seed()
        seed = random.randint(0, 2**32)
        # print(f'Using seed: {seed}')
    else:
        seed = int(set_seed)

    args = {
        'batchNum': batchNum,
        'prompts_series':split_prompts(text_prompts) if text_prompts else None,
        'image_prompts_series':split_prompts(image_prompts) if image_prompts else None,
        'seed': seed,
        'display_rate':display_rate,
        'n_batches':n_batches if animation_mode == 'None' else 1,
        'batch_size':batch_size,
        'batch_name': batch_name,
        'steps': steps,
        'width_height': width_height,
        'clip_guidance_scale': clip_guidance_scale,
        'tv_scale': tv_scale,
        'range_scale': range_scale,
        'sat_scale': sat_scale,
        'cutn_batches': cutn_batches,
        'init_image': init_image,
        'init_scale': init_scale,
        'skip_steps': skip_steps,
        'sharpen_preset': sharpen_preset,
        'keep_unsharp': keep_unsharp,
        'side_x': side_x,
        'side_y': side_y,
        'timestep_respacing': timestep_respacing,
        'diffusion_steps': diffusion_steps,
        'animation_mode': animation_mode,
        'video_init_path': video_init_path,
        'extract_nth_frame': extract_nth_frame,
        'key_frames': key_frames,
        'max_frames': max_frames if animation_mode != "None" else 1,
        'interp_spline': interp_spline,
        'start_frame': start_frame,
        'angle': angle,
        'zoom': zoom,
        'translation_x': translation_x,
        'translation_y': translation_y,
        'angle_series':angle_series,
        'zoom_series':zoom_series,
        'translation_x_series':translation_x_series,
        'translation_y_series':translation_y_series,
        'frames_scale': frames_scale,
        'calc_frames_skip_steps': calc_frames_skip_steps,
        'skip_step_ratio': skip_step_ratio,
        'calc_frames_skip_steps': calc_frames_skip_steps,
        'text_prompts': text_prompts,
        'image_prompts': image_prompts,
        'cut_overview': eval(cut_overview),
        'cut_innercut': eval(cut_innercut),
        'cut_ic_pow': cut_ic_pow,
        'cut_icgray_p': eval(cut_icgray_p),
        'intermediate_saves': intermediate_saves,
        'intermediates_in_subfolder': intermediates_in_subfolder,
        'steps_per_checkpoint': steps_per_checkpoint,
        'perlin_init': perlin_init,
        'perlin_mode': perlin_mode,
        'set_seed': set_seed,
        'eta': eta,
        'clamp_grad': clamp_grad,
        'clamp_max': clamp_max,
        'skip_augs': skip_augs,
        'randomize_class': randomize_class,
        'clip_denoised': clip_denoised,
        'fuzzy_prompt': fuzzy_prompt,
        'rand_mag': rand_mag,
    }

    args = SimpleNamespace(**args)

    print('Prepping model...')
    model, diffusion = create_model_and_diffusion(**model_config)
    model.load_state_dict(torch.load(f'{model_path}/{diffusion_model}.pt', map_location='cpu'))
    model.requires_grad_(False).eval().to(device)
    for name, param in model.named_parameters():
        if 'qkv' in name or 'norm' in name or 'proj' in name:
            param.requires_grad_()
    if model_config['use_fp16']:
        model.convert_to_fp16()

    gc.collect()
    torch.cuda.empty_cache()
    try:
        do_run()
    except KeyboardInterrupt:
        pass
    finally:
        print('Seed used:', seed)
        gc.collect()
        torch.cuda.empty_cache()


    # # 6. Create the video

    # @title ### **Create video**
    #@markdown Video file will save in the same folder as your images.

    skip_video_for_run_all = True #@param {type: 'boolean'}

    if skip_video_for_run_all == False:
      # import subprocess in case this cell is run without the above cells
      import subprocess
      from base64 import b64encode

      latest_run = batchNum

      folder = batch_name #@param
      run = latest_run #@param
      final_frame = 'final_frame'


      init_frame = 1#@param {type:"number"} This is the frame where the video will start
      last_frame = final_frame#@param {type:"number"} You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.
      fps = 12#@param {type:"number"}
      view_video_in_cell = False #@param {type: 'boolean'}

      frames = []
      # tqdm.write('Generating video...')

      if last_frame == 'final_frame':
        last_frame = len(glob(batchFolder+f"/{folder}({run})_*.png"))
        print(f'Total frames: {last_frame}')

      image_path = f"{outDirPath}/{folder}/{folder}({run})_%04d.png"
      filepath = f"{outDirPath}/{folder}/{folder}({run}).mp4"


      cmd = [
          'ffmpeg',
          '-y',
          '-vcodec',
          'png',
          '-r',
          str(fps),
          '-start_number',
          str(init_frame),
          '-i',
          image_path,
          '-frames:v',
          str(last_frame+1),
          '-c:v',
          'libx264',
          '-vf',
          f'fps={fps}',
          '-pix_fmt',
          'yuv420p',
          '-crf',
          '17',
          '-preset',
          'veryslow',
          filepath
      ]

      process = subprocess.Popen(cmd, cwd=f'{batchFolder}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)
      stdout, stderr = process.communicate()
      if process.returncode != 0:
          print(stderr)
          raise RuntimeError(stderr)
      else:
          print("The video is ready")

      if view_video_in_cell:
          mp4 = open(filepath,'rb').read()
          data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
          display.HTML("""
          <video width=400 controls>
                <source src="%s" type="video/mp4">
          </video>
          """ % data_url)

if __name__ == '__main__':
    # # 1. Set Up
    import os
    from os import path
    import sys
    from argparse import ArgumentParser, Namespace

    def run_from_ipython():
        try:
            __IPYTHON__
            return True
        except NameError:
            return False
    parser = ArgumentParser()
    parser.add_argument("--gpu", default="0", type=str)
    parser.add_argument("--text", default="", type=str)
    parser.add_argument("--root_path", default="out_diffusion")
    parser.add_argument("--setup", default=False, type=bool)
    parser.add_argument("--out_name", default="out_image", type=str)
    parser.add_argument("--sharpen_preset", default="Off", type=str, choices=['Off', 'Faster', 'Fast', 'Slow', 'Very Slow'])
    parser.add_argument("--width", default=1280, type=int)
    parser.add_argument("--height", default=768, type=int)
    parser.add_argument("--init_image", default=None, type=str)
    parser.add_argument("--steps", default=250, type=int)
    parser.add_argument("--skip_steps", default=None, type=int)
    parser.add_argument("--inter_saves", default=3, type=int)
    main(argparse_args=parser.parse_args())
